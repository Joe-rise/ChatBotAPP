

tensorflow 的系统框架：

front end ： c++ python java others：  给用户的语言接口；
C API:
很多图的计算，管理session的周期；tensorflow都是基于会话的模式；

distributed Runtime:
distributed master（用于分布式的操作；用户的一些操作会经过这个操作；） work services（是一个工作站；） dataflow executor（数据流的执行器）

kernel implements:

network layer:（网络协议）    device layer
RPC RDMA                        GPU CPU


tensorflow 的基本的要素：
1：张量
2：图（graph），每一个操作节点，构成的一个图；
3：会话：（session）一个操作从开始到结尾的过程；

张量的维度描述为阶，但是张量的阶和举证的阶并不是一个概念，张量的阶，是张量维度的一个数量描述；
张量是具有流动性；
3，也就是纯量
v = [1,1,1,1] 一阶张量，也就是向量
t = [[11,11,],[11,11]] 二阶张量

图：表示模型的数据流：由ops和tensor组成，ops相当于节点，也就是操作；tensor是数据流；也就是边；
算法都会表示成计算图（computational graphs）
也称之为数据流图；我们可以把计算图堪称一种邮箱图；张量就是通过各种操作在有向图中流动；


要想启动一个图：必须要创建一个会话；
tensorflow中所有的图都是在会话中完成的；

tensorflow基本原理及模型的训练：
开始
定义数据集
定义模型；输入和输出，如何进行处理的；（也就是网络，如rnn）
编写程序并训练模型；
    一般不会把训练数据和测试数据没有包含关系；
模型的测试；

1：训练集尽可能大；
2：训练集和测试集的结构要一致；
3：调节参数；（训练多少轮）训练的次数过多可能会导致过拟合；训练过程少可能会欠拟合；
4：损失函数：
    参数调优
    数据的优化；



Android 系统介绍；
什么是它：
是基于linux的自由及开放源代码的操作系统，主要用于移动设备；
嵌入式设备；

分层的架构：
    1：应用程序曾 ：包含了一切重要的程序：如：home，contacts phone，brower：android系统自带的必备的服务；
    2：应用程序架构层：（普通开发者需要掌握的层）
        activity manager ：活动管理：管理应用程序的生存周期
        windows manager，窗口管理接口；等其他框架接口；界面的跳动
        content providers：一个应用程序的可以访问另一个应用程序的数据；
        viewsystem：视图，界面的；各种UI组件；
        notefication manager ：通知管理
        pakage manage：包管理，操作系统的唯一性，是通过包来的
        telephony manage：手机通信：信用卡的状态，电话网络状态；
        resource manager：资源管理；
        location manager：地图管理；获取位置；
        xmpp：可扩展的服务：通信服务；



    3：系统运行库层（函数库）：如位置，媒体服务等
        surface：显示系统资源，显示系统的包；显示子系统的管理，为应用程序体统2D或者是3D的融合；
        media framework ：媒体
        SQLite：android的数据库
        OpenGLES：3D加速的库；
        freeType：向量的显示
        webkit：支持安卓浏览器；嵌入相关；
        SGL：2d图形
        SSl：安全套接层：网络连接；
        libc：为linux提供接口；

        安卓运行库：
            1：core lib：提供java编程语言的库
            2：安卓平台的虚拟机；相当于运行在linux系统上的虚拟机；类似玉windows电脑上的虚拟机功能；
    4：linux内核层
        支持安卓的最低层的东西；如电量管理；

    开发人员可以使用java语言来编写android程序，但是也可以用其他的语言进行；


安卓的开发流程：
1：开始
2：编辑布局文件：通俗讲，就是我们能看见的界面；
3：为每一个控件添加实现；如按钮；给他添加时间，(也就是i业务逻辑的实现）
4：运行调试：（在之前还有很多操作；如网络请求，蓝牙请求，权限问题）
5：打包部署：包名：为了解决包名的重复，为每一个app的签名文件，包名和签名文件进行打包部署；签名文件不同，包名一样也不会重复；
    这是一个简单的过程；资源文件，还由其他的逻辑和资源实现；


搭建android开发环境；
1：JDK ：需要配置：JAVA_HOME和CLASSPATH:%JAVA_HOME%\jre\lib\rt.jar
2：android SDK
3：android studio :本地没有就自己设置一个目录进行下载；s

androidstudio的使用：
activity name ：第一个提供界面的类的名字；
使用，对于报错的安装一下工具就可以了；下载有点慢；
点击运行这时候报错没有设备，这时候进入mumu模拟器的安装路径进入：上一级目录进入vmonitor -》bin 复制文件路径，然后打开dos命令，切换到这个目录
通过命令adb_server connect 127.0.0.1:7555
这时候在androidstudio里面启动就不会报错了；
接下来有个错误是Android 应用， 手机调试 minSdk（API 23）> deviceSdk（api 17
这时候需要更改
Android Studio(2.1.2)新建工程的时候只会让你选择最低支持的SDK版本，默认的目标编译SDK版本会以系统当前SDK中最新SDK platform作为目标的API Level。但是很多时候我们并不需要最新的SDK版本，如何修改呢？
方法是：修改工程目录中的Gradle Scripts->build.gradle(Module:app)中的相关行，具体见下图：
有一个minsdkversion的东西，更改之后会让你重新build一遍，之后就可以了；

layout name : 布局文件的名字

androidstudio调试google；
其他的android模拟器；比google的快一些；

NLP基础：
常用神经网络模型：
1：CNN也是前馈神经网络的一种；图片修复上色；
2：循环神经网络：recurrent neural network 这种网络内部状态可以展示攻台时序行为；
3：LSTM：长短期记忆网络；他是循环神经网络的一种变体：适合于处理预测时间序列中间隔和延迟相对长的重要时间；
    聊天机器人需要用到；
循环神经网络：

    传统的感知机模型：就是y = w1x1 +w2x2.....+wn*xn
    BP神经网络：（也是前馈神经网络）
        输入层：数据模型的输入，即传入模型的数据；
        隐藏层：用于处理数据，并将处理的结构传递给输出层；
        输出层：就是经过隐藏层处理之后得到的输出；


BP神经网络：
1：正向传播
    1：网络初始化：就是初始化权重值；权重，和偏置项做定义；
    2：隐藏层的输出
    3：输出层的输出 其实就是y = wx+b ,w和x，b都是矩阵；
2：误差计算（误差的定义，一般是输出层的输出于真实值的“差”）
3：反向传播；（也就是误差反向传播）通过链式法则来实现的；求的偏导；
    会对权重值进行调节（通过求导的方式，）
    隐藏层到输出层：更新这一部分的权重值
    2：输入层到隐藏层：更新这一部分的权重

4：偏置更新
    也叫偏移更新：
    各个层更新各自的
    隐藏层到输出层
    输入层到隐藏层的

可以通过逐层信息传递到最后的输出
沿着一条直线计算；知道最后一层，求出计算结果；
包含输入层，输出层和影藏曾；其目的就是实现从输入到输出的映射
一般包含多层，并且曾于曾之间是全连接，不存在同层和跨层连接；


循环神经网络：
循环神经网络最重要的是一个环状结构，
是一个序列形式的结构；

每一个节点都有两个输入，和两个输出；
因为他有上一个时刻和下一个时刻：

循环的时候有一个权重；w
循环神经网络中有w，v，u，每一个时刻他们都是相同，参数共享；

三个特性：
    1：记忆；记住上一个
    2：接受两个参数
    3：参数共享（确保每一步做相同的事



于经典神经网络的对比：
循环神经网络：w参数是不变的；


循环神经网络有很多结构：
1：one to one  分类问题：可以使用这个
2:one to many ：图片描述；音乐的生成
3:many to one：  多分类的问题；句子的输入
4:many to many(输入和输出维度不同） 翻译
5：many to many (输入和输出维度相同）句子的识别，论文识别；


实例：
    明天早上是英语课，所以我们明天要带 ______书；
    传统我们是通过循环神经网络，计算每一个词出现的概率；计算一个权重，然后累成起来，最后得到我们的结果；

每一个循环的节点都有一个权重：循环的过程，权重是累乘的；


    明天早上是英语课，所以我们明天要带 ______书；
    简单结构是对文字编码：要想得到结构，我们需要每一个时刻的权重值；由于权重值的共享；权重累成；
    这个循环神经网络：都是从左向右，不能返回：
    但是我们填空；也需要通过后面的内容来预测前面：

    双向循环神经网络，能实现前面预测后面，后面预测前面；也就是前面和后面是同时进行；
    但是只有单项参数是共享的；

    向前传播和向后传播是不影响的；分开的；分开计算的；
    双向循环神经网络的特点：
        1：每个时刻有两个影藏曾；
        2：向前传播向后传播不影响
        3：一个从左到右，一个从右到左；


    每一次的w都会被累计相乘；
    这就有一个问题：如果w非常小，一直类乘，会出现w消失的情况：也就是梯度消失
    那么非常大，会出现梯度爆炸

    必须要解决梯度消失和梯度爆炸的情况：
    权重w一般权重不为1；为1是非常理想的情况；
    想办法解决梯度爆炸和梯度消失的问题；



    解决梯度消失和梯度爆炸的方法：
        1：选择合适的激活函数；如relu和sigmoid，sigmoid就容易出现梯度消失的情况；
        2：选择合适的参数初始化方法；
        3：选择权重参数的正则化
        4：使用batchnormalization
        5：使用残差结构
        6：使用梯度裁剪


    1：选择合适的激活函数：
        误差在反向传播的过程求导的时候，relu函数倒数为1，那么就特别理想：
        选择的激活函数为relu函数，
        但是不建议使用sigmoid，和tanh，因为他们的导数在大部分情况下非常小；很容易导致梯度消失问题；
            为什么不选择他们呢：
                1：区域小
                2：倒数特别小，容易导致梯度消失；

     2：选择合适的参数初始化
        误差在传递的过程中；参数要合理，不然相乘会出现很多问题；
        最好的参数初始化方法：为：
            np.random.randn(shap[l])*0.01;乘以0.01会打破参数对称的问题：但是对于大型的会出现梯度消失问题；
         第二种初始化方法：
            w = np.random.randn(shape)np.sqrt(1/n^l-1);
            选择合适的参数初始化也是很合适的方式；


        使用：batchnormalization：
            好处：
                1：使用规范化的操作将输出信号规范化到均值为0，方差为1，保证网络的稳定性；
                2：加跨网络的训练速度（加大收敛的效果）
                3：提高网络的稳定性
                4：可以有效的解决梯度爆炸和梯度消失问题；

         在做误差的反向传播过程中，都会乘以改乘的权重，那么规范化的手段，使偏离的分布拉回；


         3使用残差网络：
            残差网络：加大网络的深度，使模型更加复杂；
            残差网络可以轻松的构造几百层的神经网络；
            残差有个high way的东西，可以跨层；
            梯度是绕一圈，但是权重是要经过神经网络层的；那么可以解决权重的问题；
                1：极大的提高的神经网络深度
                2：很大程度上解决梯度消散的问题；
                3：允许我们可以训练很深层的网络
                4残差结构可以看作解决梯度消失最有效的，最终要的方法；

         4：使用梯度裁剪的方法：
            通过截面悬崖的的时候有时候可能会直接消失；
            通过裁剪，可以减缓它的梯度问题；

            其中v是梯度范数的上界；g用来更新参数的梯度；
            if ||g|| >v
            g<-v



LSTM  :
    rnn的方法得到叠被的过程；
    如果叠被子没有出现在后面，那么会导致它认为它这个词不重要；
    就会觉得叠被子不是关键字；
    rnn对于较长序列的记忆问题；

    那么我们的解决办法就是：
        LSTM：
        rnn和LSTM的结构：差别：
            1：LSTM多了一个C：也就是用来存储记忆的状态：
            把lstm的展开：
            lstm接受三个参数，相比于rnn多了一个记忆状态c：
             利用们的一种方式来控制状态c
                三种门：
                    1：输入门
                    2：输出门
                    3：遗忘门
             循环单元结构中：
                输入数据之后，需要进行判断：有一个tanh，
                遗忘门：上一时刻状态的留或者丢失；

                还有一种GRU的一种方式：
                    他也是利用门的方式：但是门不一样：
                        1：重置门
                        2：更新门：用来考虑权重的跟新，损失的判断等；
                GRU：于LSTM的区别：

        LSTM有一个高速公路的东西：
            直接把损失直接传回去：不经过激活函数：
            这样可以对于记忆的保存；




NLP的基础：
什么是NLp
RNN和LSTM
NLP语言模型：
NLP模型的建立




NLP：自然语言处理有两种，文字，和语音；

包括：文本分析；
信息检索；如搜索引擎，不是单独的检索的功能；
词性标注，：也就是，动词，名词等就是词性
问答系统；

邮件回复系统；



NLP基础：
1词法分析
    1分词技术
    2：词性标注
        就是把某一些词放在一起，按照形容词，或者动词来分类；
        也就是按照单词的词性把单词分类；
        把这些分类好的词用一些标记方法注上标记；
        标记一个正确词性的过程叫词性标注；
        如，的“，汉语词典，中科院有一个研究成果；可以进行分词；并进行词性标注；
        在：ictclas.nlpir.org/nlpir

    3：命名实体识别：人名，地名，特殊的我们需要的识别出来的东西
    4：词义消歧：一个词具有多种含义；也就是消除这个歧义；
    如：今天的天器分厂晴朗，我和小伙伴门一起去足球场踢足球；
    读出这句话：断句要好；我们要通过深度学习，机器学习的方法进行分词；
2：句法分析
3：语法分析；

跟nlp相关的都离不开这些；





























