

tensorflow 的系统框架：

front end ： c++ python java others：  给用户的语言接口；
C API:
很多图的计算，管理session的周期；tensorflow都是基于会话的模式；

distributed Runtime:
distributed master（用于分布式的操作；用户的一些操作会经过这个操作；） work services（是一个工作站；） dataflow executor（数据流的执行器）

kernel implements:

network layer:（网络协议）    device layer
RPC RDMA                        GPU CPU


tensorflow 的基本的要素：
1：张量
2：图（graph），每一个操作节点，构成的一个图；
3：会话：（session）一个操作从开始到结尾的过程；

张量的维度描述为阶，但是张量的阶和举证的阶并不是一个概念，张量的阶，是张量维度的一个数量描述；
张量是具有流动性；
3，也就是纯量
v = [1,1,1,1] 一阶张量，也就是向量
t = [[11,11,],[11,11]] 二阶张量

图：表示模型的数据流：由ops和tensor组成，ops相当于节点，也就是操作；tensor是数据流；也就是边；
算法都会表示成计算图（computational graphs）
也称之为数据流图；我们可以把计算图堪称一种邮箱图；张量就是通过各种操作在有向图中流动；


要想启动一个图：必须要创建一个会话；
tensorflow中所有的图都是在会话中完成的；

tensorflow基本原理及模型的训练：
开始
定义数据集
定义模型；输入和输出，如何进行处理的；（也就是网络，如rnn）
编写程序并训练模型；
    一般不会把训练数据和测试数据没有包含关系；
模型的测试；

1：训练集尽可能大；
2：训练集和测试集的结构要一致；
3：调节参数；（训练多少轮）训练的次数过多可能会导致过拟合；训练过程少可能会欠拟合；
4：损失函数：
    参数调优
    数据的优化；



Android 系统介绍；
什么是它：
是基于linux的自由及开放源代码的操作系统，主要用于移动设备；
嵌入式设备；

分层的架构：
    1：应用程序曾 ：包含了一切重要的程序：如：home，contacts phone，brower：android系统自带的必备的服务；
    2：应用程序架构层：（普通开发者需要掌握的层）
        activity manager ：活动管理：管理应用程序的生存周期
        windows manager，窗口管理接口；等其他框架接口；界面的跳动
        content providers：一个应用程序的可以访问另一个应用程序的数据；
        viewsystem：视图，界面的；各种UI组件；
        notefication manager ：通知管理
        pakage manage：包管理，操作系统的唯一性，是通过包来的
        telephony manage：手机通信：信用卡的状态，电话网络状态；
        resource manager：资源管理；
        location manager：地图管理；获取位置；
        xmpp：可扩展的服务：通信服务；



    3：系统运行库层（函数库）：如位置，媒体服务等
        surface：显示系统资源，显示系统的包；显示子系统的管理，为应用程序体统2D或者是3D的融合；
        media framework ：媒体
        SQLite：android的数据库
        OpenGLES：3D加速的库；
        freeType：向量的显示
        webkit：支持安卓浏览器；嵌入相关；
        SGL：2d图形
        SSl：安全套接层：网络连接；
        libc：为linux提供接口；

        安卓运行库：
            1：core lib：提供java编程语言的库
            2：安卓平台的虚拟机；相当于运行在linux系统上的虚拟机；类似玉windows电脑上的虚拟机功能；
    4：linux内核层
        支持安卓的最低层的东西；如电量管理；

    开发人员可以使用java语言来编写android程序，但是也可以用其他的语言进行；


安卓的开发流程：
1：开始
2：编辑布局文件：通俗讲，就是我们能看见的界面；
3：为每一个控件添加实现；如按钮；给他添加时间，(也就是i业务逻辑的实现）
4：运行调试：（在之前还有很多操作；如网络请求，蓝牙请求，权限问题）
5：打包部署：包名：为了解决包名的重复，为每一个app的签名文件，包名和签名文件进行打包部署；签名文件不同，包名一样也不会重复；
    这是一个简单的过程；资源文件，还由其他的逻辑和资源实现；


搭建android开发环境；
1：JDK ：需要配置：JAVA_HOME和CLASSPATH:%JAVA_HOME%\jre\lib\rt.jar
2：android SDK
3：android studio :本地没有就自己设置一个目录进行下载；s

androidstudio的使用：
activity name ：第一个提供界面的类的名字；
使用，对于报错的安装一下工具就可以了；下载有点慢；
点击运行这时候报错没有设备，这时候进入mumu模拟器的安装路径进入：上一级目录进入vmonitor -》bin 复制文件路径，然后打开dos命令，切换到这个目录
通过命令adb_server connect 127.0.0.1:7555
这时候在androidstudio里面启动就不会报错了；
接下来有个错误是Android 应用， 手机调试 minSdk（API 23）> deviceSdk（api 17
这时候需要更改
Android Studio(2.1.2)新建工程的时候只会让你选择最低支持的SDK版本，默认的目标编译SDK版本会以系统当前SDK中最新SDK platform作为目标的API Level。但是很多时候我们并不需要最新的SDK版本，如何修改呢？
方法是：修改工程目录中的Gradle Scripts->build.gradle(Module:app)中的相关行，具体见下图：
有一个minsdkversion的东西，更改之后会让你重新build一遍，之后就可以了；

layout name : 布局文件的名字

androidstudio调试google；
其他的android模拟器；比google的快一些；

NLP基础：
常用神经网络模型：
1：CNN也是前馈神经网络的一种；图片修复上色；
2：循环神经网络：recurrent neural network 这种网络内部状态可以展示攻台时序行为；
3：LSTM：长短期记忆网络；他是循环神经网络的一种变体：适合于处理预测时间序列中间隔和延迟相对长的重要时间；
    聊天机器人需要用到；
循环神经网络：

    传统的感知机模型：就是y = w1x1 +w2x2.....+wn*xn
    BP神经网络：（也是前馈神经网络）
        输入层：数据模型的输入，即传入模型的数据；
        隐藏层：用于处理数据，并将处理的结构传递给输出层；
        输出层：就是经过隐藏层处理之后得到的输出；


BP神经网络：
1：正向传播
    1：网络初始化：就是初始化权重值；权重，和偏置项做定义；
    2：隐藏层的输出
    3：输出层的输出 其实就是y = wx+b ,w和x，b都是矩阵；
2：误差计算（误差的定义，一般是输出层的输出于真实值的“差”）
3：反向传播；（也就是误差反向传播）通过链式法则来实现的；求的偏导；
    会对权重值进行调节（通过求导的方式，）
    隐藏层到输出层：更新这一部分的权重值
    2：输入层到隐藏层：更新这一部分的权重

4：偏置更新
    也叫偏移更新：
    各个层更新各自的
    隐藏层到输出层
    输入层到隐藏层的

可以通过逐层信息传递到最后的输出
沿着一条直线计算；知道最后一层，求出计算结果；
包含输入层，输出层和影藏曾；其目的就是实现从输入到输出的映射
一般包含多层，并且曾于曾之间是全连接，不存在同层和跨层连接；


循环神经网络：
循环神经网络最重要的是一个环状结构，
是一个序列形式的结构；

每一个节点都有两个输入，和两个输出；
因为他有上一个时刻和下一个时刻：

循环的时候有一个权重；w
循环神经网络中有w，v，u，每一个时刻他们都是相同，参数共享；

三个特性：
    1：记忆；记住上一个
    2：接受两个参数
    3：参数共享（确保每一步做相同的事



于经典神经网络的对比：
循环神经网络：w参数是不变的；


循环神经网络有很多结构：
1：one to one  分类问题：可以使用这个
2:one to many ：图片描述；音乐的生成
3:many to one：  多分类的问题；句子的输入
4:many to many(输入和输出维度不同） 翻译
5：many to many (输入和输出维度相同）句子的识别，论文识别；


实例：
    明天早上是英语课，所以我们明天要带 ______书；
    传统我们是通过循环神经网络，计算每一个词出现的概率；计算一个权重，然后累成起来，最后得到我们的结果；

每一个循环的节点都有一个权重：循环的过程，权重是累乘的；


    明天早上是英语课，所以我们明天要带 ______书；
    简单结构是对文字编码：要想得到结构，我们需要每一个时刻的权重值；由于权重值的共享；权重累成；
    这个循环神经网络：都是从左向右，不能返回：
    但是我们填空；也需要通过后面的内容来预测前面：

    双向循环神经网络，能实现前面预测后面，后面预测前面；也就是前面和后面是同时进行；
    但是只有单项参数是共享的；

    向前传播和向后传播是不影响的；分开的；分开计算的；
    双向循环神经网络的特点：
        1：每个时刻有两个影藏曾；
        2：向前传播向后传播不影响
        3：一个从左到右，一个从右到左；


    每一次的w都会被累计相乘；
    这就有一个问题：如果w非常小，一直类乘，会出现w消失的情况：也就是梯度消失
    那么非常大，会出现梯度爆炸

    必须要解决梯度消失和梯度爆炸的情况：
    权重w一般权重不为1；为1是非常理想的情况；
    想办法解决梯度爆炸和梯度消失的问题；



    解决梯度消失和梯度爆炸的方法：
        1：选择合适的激活函数；如relu和sigmoid，sigmoid就容易出现梯度消失的情况；
        2：选择合适的参数初始化方法；
        3：选择权重参数的正则化
        4：使用batchnormalization
        5：使用残差结构
        6：使用梯度裁剪


    1：选择合适的激活函数：
        误差在反向传播的过程求导的时候，relu函数倒数为1，那么就特别理想：
        选择的激活函数为relu函数，
        但是不建议使用sigmoid，和tanh，因为他们的导数在大部分情况下非常小；很容易导致梯度消失问题；
            为什么不选择他们呢：
                1：区域小
                2：倒数特别小，容易导致梯度消失；

     2：选择合适的参数初始化
        误差在传递的过程中；参数要合理，不然相乘会出现很多问题；
        最好的参数初始化方法：为：
            np.random.randn(shap[l])*0.01;乘以0.01会打破参数对称的问题：但是对于大型的会出现梯度消失问题；
         第二种初始化方法：
            w = np.random.randn(shape)np.sqrt(1/n^l-1);
            选择合适的参数初始化也是很合适的方式；


        使用：batchnormalization：
            好处：
                1：使用规范化的操作将输出信号规范化到均值为0，方差为1，保证网络的稳定性；
                2：加跨网络的训练速度（加大收敛的效果）
                3：提高网络的稳定性
                4：可以有效的解决梯度爆炸和梯度消失问题；

         在做误差的反向传播过程中，都会乘以改乘的权重，那么规范化的手段，使偏离的分布拉回；


         3使用残差网络：
            残差网络：加大网络的深度，使模型更加复杂；
            残差网络可以轻松的构造几百层的神经网络；
            残差有个high way的东西，可以跨层；
            梯度是绕一圈，但是权重是要经过神经网络层的；那么可以解决权重的问题；
                1：极大的提高的神经网络深度
                2：很大程度上解决梯度消散的问题；
                3：允许我们可以训练很深层的网络
                4残差结构可以看作解决梯度消失最有效的，最终要的方法；

         4：使用梯度裁剪的方法：
            通过截面悬崖的的时候有时候可能会直接消失；
            通过裁剪，可以减缓它的梯度问题；

            其中v是梯度范数的上界；g用来更新参数的梯度；
            if ||g|| >v
            g<-v



LSTM  :
    rnn的方法得到叠被的过程；
    如果叠被子没有出现在后面，那么会导致它认为它这个词不重要；
    就会觉得叠被子不是关键字；
    rnn对于较长序列的记忆问题；

    那么我们的解决办法就是：
        LSTM：
        rnn和LSTM的结构：差别：
            1：LSTM多了一个C：也就是用来存储记忆的状态：
            把lstm的展开：
            lstm接受三个参数，相比于rnn多了一个记忆状态c：
             利用们的一种方式来控制状态c
                三种门：
                    1：输入门
                    2：输出门
                    3：遗忘门
             循环单元结构中：
                输入数据之后，需要进行判断：有一个tanh，
                遗忘门：上一时刻状态的留或者丢失；

                还有一种GRU的一种方式：
                    他也是利用门的方式：但是门不一样：
                        1：重置门
                        2：更新门：用来考虑权重的跟新，损失的判断等；
                GRU：于LSTM的区别：

        LSTM有一个高速公路的东西：
            直接把损失直接传回去：不经过激活函数：
            这样可以对于记忆的保存；




NLP的基础：
什么是NLp
RNN和LSTM
NLP语言模型：
NLP模型的建立




NLP：自然语言处理有两种，文字，和语音；

包括：文本分析；
信息检索；如搜索引擎，不是单独的检索的功能；
词性标注，：也就是，动词，名词等就是词性
问答系统；

邮件回复系统；



NLP基础：
1词法分析
    1分词技术
    2：词性标注
        就是把某一些词放在一起，按照形容词，或者动词来分类；
        也就是按照单词的词性把单词分类；
        把这些分类好的词用一些标记方法注上标记；
        标记一个正确词性的过程叫词性标注；
        如，的“，汉语词典，中科院有一个研究成果；可以进行分词；并进行词性标注；
        在：ictclas.nlpir.org/nlpir

    3：命名实体识别：人名，地名，特殊的我们需要的识别出来的东西
    4：词义消歧：一个词具有多种含义；也就是消除这个歧义；
    如：今天的天器分厂晴朗，我和小伙伴门一起去足球场踢足球；
    读出这句话：断句要好；我们要通过深度学习，机器学习的方法进行分词；
2：句法分析
3：语法分析；

跟nlp相关的都离不开这些；

语意分析和情感分析都需要词性标注；
分词，然后词性标注后再训练；

词性标注或者标注；
对文本处理；此役解析，都会用到词性标注这个东西；

命名实体识别：识别文本中具有特定意义的实体，如任命，地名，机构名，专有名词；

命名实体是信息提取，问答系统，句法分析；机器翻译，面向semantic web的元数据标注等应用领域的重要基础工具；再自然语言处理走向实用化的过程中重要的地位；


三大类，七小类：
    实体类
    时间类数字类
        7：人名，机构名，地名，时间，日期，货币，百分比；


命名实体识别：
    实体边界识别：可以看成分词；
    确定实体类别： 英文实体  中文实体；

    中文首先要进行分词：
        词的切分：
            基于规则和词典的方法：今天天气特别好；他是从末尾依此去掉一个字后，在词典中查，看一下有没有这个词；基于词典或者是规则的方法是不科学的；
            基于统计的方法;目前比较主流的方法：
                1：HMM
                2：ME ：较大熵
                3：SVM
                4：CRF：条件学习场

贝叶斯发分类：
    算法：统计学的一种分类方法:,他是根据概率统计知识进行的；
    朴素贝叶斯是基于贝叶斯定理于特征假设的分类方法；
        最为普片的分类模型由：决策树模型：朴素贝叶斯模型；

    在许多场合下，朴素贝叶斯和决策树和神经网络向媲美；
        该分类算法运用到大型数据苦衷，而且方法简单，分类准确率高，速度快；
    长文本或者大型文本很多时候考虑朴素贝叶斯；

    贝叶斯就是已知一件事发生的情况下，某一个条件发生的概率；
    p(x|c):也就是已知c发生，那么x发生的概率是多少；这是后验概率；
    p(x)为先验概率；可以预估的概率；
    后验概率：在很多情况出现情况下的概率；

    朴素贝叶斯：就是目标值时，属性之间相互独立；
    p(c|x) = p(c)p(x|c)/p（x):把多个条件概率相乘计算；
    朴素贝叶斯一般比贝叶斯效果要好一些；

    朴树贝叶斯算法的常用场景：
        1：文本分类：正面信息，负面信息；某些公司多文章进行多分类；
        2：垃圾邮件过滤
        3：多分类实时预测：加入和实时的预测；
        4：拼写纠错；文章编写中是否出错；

    垃圾邮件过滤，
    我司可带开普通增值税发票，税点优惠，欢迎来电咨询；
    ocr将图片转化成文字；

    最终得到的是一个概率；
    LR：也是一种分类技术


    马尔科夫过程：是一类随机过程：
    在已知目前状态（现在）的条件下；它位来的演变（将来）不依赖于它以往的演变（过去）；主要研究一个系统的状态；
    机器转移的理论，他是通过对不同状态的初始概率以及状态之间的转移概率的研究，来去欸的那个状态的变化和确实；从而达到预测位来的目的；

    无后效性；
    便利性：

    马尔科夫链：具有马尔科夫性质的离散时间过程，即时间和状态参数都是，离散的马尔科夫过程；
    隐马尔科夫模型（HMM）作为一种统计分析模型
        结构做简单的动态贝叶斯网，这是一种著名的有向图模型，主要用于时序数据模型（语音识别，自然语言处理）
    时间序列就是有方向的，可以用有向图模型建模；

    隐马尔科夫模型是马尔科夫链的一种他的状态不能直接观察到；但是通过观测向量时序观察到，每个观测向量都是通过某些概率分布表现为各种状态；
        每一个观测向量由一个具有相应概率分布的状态序列产生；


    z0 - z1 - z2 -...zt:状态的变量 第t个时刻的状态（隐变量）
    x0 - x1 - x2 ....xt：第i个时刻观测到的一个值；n个取值的状态空间；离散型或者是连续型；
    箭头表示了，变量之间的依赖关系；
    他是一个链式的状态，也就是下一个状态仅依赖于当前时刻；
    状态的变量是隐藏的；
    隐马尔科夫模型在每一个状态都有一个值；

    具体什么构成：两个状态集合）三个概率矩阵
        1：N:模型的状态数，状态时可是可以转移的；
        2：M：每个状态不同的观测符号；即输出字符的个数；
        3：A：状态转移的概率；
        4：B：观察符号在各个状态下的概率分布；
        5pi，表示初始状态分布；

        输入：HMMs(N,M,A,B,pi)
        输出：一个观察符号的序列，这个序列的每个元素都是M中的元素；


        它的应用场景：
            中文分词
            机器翻译
            语音识别
            通信中的译码

            传统的机器学习模型：
                很多深度学习模型需要依赖机器学习模型；
            马尔科夫随机场：无向图模型；

        命名实体的识别：
            1：语料--》》》训练：词性作为观察的值
                初始概率矩阵：pi
                转移概率矩阵：B
                发射概率：A（状态转移的概率）
            2：NE识别
                分词且标注词性的句子
            3：规则修正
            4：标注转换
                标注了NE标记的句子

            特定的词它表现的不好，所以要有命名实体识别；


朴素贝叶斯例子：
    jieba
    sklearn
    scipy






预料的获取和处理：
语料，即语言材料，语料是语言学研究的内容；语料是构成语料库的基本单元；
语料库中存放的是语言的实际使用过程中真实出现过的语言材料；

语料是真实能表达真实信息的；
语料库事宜电子计算机为i载体的语言知识的基础资源；

语言标准；

真实语料需要加工（分析和处理），才能成为有用的资源；

语料库的种类：
异质的；
同质的；
系统的；
专用的；1


开放语料数据集：
    1：中科院自动化所的中英文新闻语料库
    2：搜狗的中文新闻语料库
    3：人工生成的机器阅读理解数据集
    4：一个开放问题于回答的挑战数据集；


    语料的处理：
        1：获取语料
        2：格式化文本
        3：特征工程；


NLP中的语言模型：
    语言模型是自然语言处理的一大利器，是NLP领域一个基本却又重要的任务，它的主要功能就是计算一个词语序列工程一个句子的概率，或者是计算一个词语序列的联合概率；这可以用来判断一句话出现的概率高不高，符不符合我们的表达习惯，他是否通顺，这句化是不是正确的；

    我是NLP开发工程师；
    开发工程师我是NLP

    NLp的语言模型：
        1：unigram models（医院文法统计模型）
        2：n_gram 语言模型：（N元模型）
            其实就是概率语言模型；

  概率语言模型：
  1：预测字符串的概率：判断一句话是否完整或者真确；
  2：动机：词频率计算，或者是做什么
  3：如何计算：以什么方式计算：贝叶斯分类；其他的一些概率模型；

经过这些步骤之后，可以计算出结构：

一元文法统计模型；是n元的特殊情况；
p(s）=p(w1) +p(w2)...p(wn)

这个式子成立的条件是有一个假设，就是脚尖无关假设，我们认为每个词都是条件无的；

二元文法统计模型：
    我喝水
    我吃水
    p(我喝水） = p(我） + p(喝） + p(水)
    p(我吃水） = p(我） + p(吃） + p(水）
    概率差别不大，实际意思却不同：

    那么什么是二元模型呢：
        二元模型：能更好的获取到两个词语之间的关系；
        能够得到上下文的关系i；

        N元模型：

        是不是N越大效果越好呢？

        当n大于n大于3的时候就无法处理了，参数空间太大，另外它布不能表示词与词之间的关联性；

        n元模型的原则上是越大越好，但是n大于3就不是很好了‘




词向量于word2vec：
    词嵌入：一组语言模型特征学习技术的特征；
        把来自词汇表或短语被映射到实数的向量；
        word2vec，是一群用来长生词向量的相关模型；这些模型为浅而

        word2vec：
            1：CBOW：CBOW模型由输入层，输出层，映射层共同构成

            2：CBOW是一个二叉树

           3：  hierarchical softmax
softmax用于大批量的语料的处理很困难的；

          是一个哈夫曼数；


skip-gram：真好和CBOW相反：他的输入时一个，输出是多个，
而CBOW模型是输入多个输出一个；但是真题上也是一个二叉树的结构；
结构是相同的，但是又有点不同；


文本处理方法：
    1：数据清洗（去掉无意义的标签，url，符号）
    2：分词，大小写转换，添加句首，词性标注；
    3：对于一句话有明确的意义的时候需要；
    4：统计词频，抽取文本特征（传统的自然语言学习，需要自己去抽取特征），特征选择（抽取到的特征需要进行一定的选择），计算特征权重，归一化；
    5：划分训练集和测试集（是我们爬取的数据）（最初的时候需要进行处理）训练集和测试集要进行一般7:3的划分；这不是按照数量进行随意划分的，需要按照类别的数据进行百分比划分；
    6：最后的训练，预测；


数据的收集：
    环境：
    1：sys：python系统库
    2：pickle：特有的类型和python数据类型进行转换；tensorflow的save或者restore只能针对tensorflow的类型；但是pickle功能更加强大；
    3：re：正则表达式的库
    4：tqdm：是一种进度条；进度条的化就用这种方式；


语料处理流程：
    1：语料收集；是问答对：聊天记录，电影对话，台词片段；
    2：语料的清洗：多余的空格，不正规的符号；多余的字符，英文；
    3：清洗的方法：正则化，切分，好坏语句判断；
    4：句子向量的编码化：
        原始文本不能直接训练
        将句子转化成向量
        将向量转化成句子；

    5：语料问答对的构建
        问答对的处理和拆分
        早上好；
        早安
        问：         答：
        早上好        早安；
     语料模型的保存：这是python的模型格式
     使用pickle保存模型；
     生成pki格式
     利用pkl格式进行语料的训练
        ---------------》》深度模型
                ------------------打包成restful；



聊天机器人模型
seq2seq模型
注意力机制
3：聊天机器人根据对话的产生方式；


seq2seq模型：
    是一个encode-decode结构的网络；他的输入是要给序列，输出也是一个序列：
    encode中将一个可变长度的信号序列变成固定长度的向量表达；decoder将这个固定长度的长度的向量变成可变长度目标的信号序列；
    这种结构最重要的地方在于输入序列和输出序列的长度是可变的；
    可以用于翻译，聊天机器人，句法分析；文本摘要；

    encoder的过程：

        1:取得输入文本，进行embedding
        2：传入lstm中进行训练
        3：记录状态，并输出当前cell结果，
        4：依此循环，得到最终结构；

    decoder的过程：
        1：在encoder最后一个时间补偿的隐藏层之后输入decoder的第一个cell里；
        2：通过激活函数得到候选文本
        3：筛选出可能性最大的文本最为下一个时间步长的输入；
        4:依此循环，得到目标；

rnn是常用的编码器方式；
通过c这个编码器得到一些东西传入rnn中的解码器；


seq2seq结构：

A ,B c  --->>

seq2seq 的缺点：只能获取上一个cell的内容，不能获取再向上的内容；
这时候可以引入注意力机制；
attention机制：
注意力机制：是在序列到序列模型中用于注意编码器状态的最常方法，它可以回顾模型的过去状态；
不仅能用来处理编码器和前面隐藏层的内容；他同时还能用来获取其他特征的分布，例如阅读理解任务中作为文本的词向量；


为什么映入注意力机制：
    减少处理高维输入数据的计算负担，公国结构化的选取输入的子集，降低数据维度；
    让处理任务系统专注于找到输入数据中显著的与当前输出相关的游泳的信息；从而提高输出的质量；
    帮助解码器这种框架；学到内容模型之间的相互关系；从而更好地表示这些信息，客服其无法解释从而很难设计的缺陷；


    如何做的呢？
        1：encode进行编码：
        2：attention
        3：decoder

        没有attension机制：是全部输入；无法捕捉上上层的隐藏层；
        他是统一进行decoder，准确度是没有那么高的；


        聊天机器人根据对话的长生方式：
        1：基于检索的模型：
            有一个数据库；放了很多问答对；
            基于问答数据集来的；
            设计这种模型：希望包含我们想到的问答对；
            缺点：有重复的语句：这时候就需要做语意分析；然后在找到可能的回答，然后计算概率返回答案；
                1：有明确的问答对数据库
                3：使用语句匹配形查找答案；
                3：答案相对固定，且很少出现语法错误；
                4：不会出现新的语句；
                5：对于行业话的语句不会乱打；（可以做专家库）
        2：基于生成式的模型；
            1：不依赖预先设定的问答库；
            2：通常基于机器翻译技术；
            3：需要大量的语料进行训练；
            4：训练的轮次太小不好，需要大量的训练次数；
            基于生成式模型也是通过：encoder和decode来做的；
            5：智能问答系统会基于生成式模型：
                但是现在还不成熟；


            基于生成式模型的：
                通过注意力的处理收集以前的信息；
                训练的要求，数据的要求也高；
                目前来讲效果还不是特别好；

                基于检索的只能用特定的数据进行；
                不能基于特定数据之外的数据；

                生成式模型和检索模型：
                    这时候需要混合模式；
                    兼具检索模式和生成模式；
                    目前最常用的解决方案；
                    检索模式长生候选数据集；
                    生成模式长生最终答案；

                优势在于：检索模式已经长生很多数据集了；
                所以来说，我们一般用的混合模型；

                我们的聊天机器人：
                    1：语音识别--》自然语言识别:---->>>对话管理
                    使用的科大讯飞的方式识别和生成语音；

                开始————处理训练的语料；加载完成------对话训练---存储训练结果；







































